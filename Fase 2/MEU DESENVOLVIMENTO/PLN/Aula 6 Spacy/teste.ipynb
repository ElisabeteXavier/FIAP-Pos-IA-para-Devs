{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7cc43fb",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceb1b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "artigo_treino = pd.read_csv(\"../Aula 5 /treino.csv\")\n",
    "artigo_teste = pd.read_csv(\"../Aula 5 /teste.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f56d4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U pip setuptools wheel\n",
    "# pip install -U spacy\n",
    "# python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "996db166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f264c89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"Adoro a cidade de Caldas novas\"\n",
    "\n",
    "doc = nlp(texto)\n",
    "\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85bad3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Adoro, Caldas novas)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a134fdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fff72e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].is_alpha # é alphanumerico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f43511fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].is_stop #Verifica se é uma stopWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02c51682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tratar_textos(doc): #tirar_stopwords e numeros \n",
    "    tokens_validos = []\n",
    "    for token in doc:\n",
    "        e_valido = not token.is_stop and token.is_alpha\n",
    "        if e_valido:\n",
    "            tokens_validos.append(token.text)\n",
    "    if len(tokens_validos) > 2: #word2vec precisa no minimo uma frase com 3 palavras para o contexto\n",
    "        return \" \".join(tokens_validos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c16fa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adoro cidade Caldas'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"Adoro 255255 cidade de Caldas Novas!\"\n",
    "doc = nlp(texto)\n",
    "tratar_textos(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32b12115",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_para_tratamento = (titulos.lower() for titulos in artigo_treino.title)\n",
    "\n",
    "textos_tratados = [tratar_textos(doc) for doc in nlp.pipe(textos_para_tratamento,\n",
    "                                                         batch_size=100,\n",
    "                                                         n_process= -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01eeed50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>polêmica marine le pen abomina negacionistas h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>macron le pen turno frança revés siglas tradic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apesar larga vitória legislativas macron terá ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>governo antecipa balanço alckmin anuncia queda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queda maio atividade econômica sobe junho bc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titulo\n",
       "0  polêmica marine le pen abomina negacionistas h...\n",
       "1  macron le pen turno frança revés siglas tradic...\n",
       "2  apesar larga vitória legislativas macron terá ...\n",
       "3  governo antecipa balanço alckmin anuncia queda...\n",
       "4       queda maio atividade econômica sobe junho bc"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titulos_tratados = pd.DataFrame({\"titulo\": textos_tratados})\n",
    "titulos_tratados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "192c83b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_modelo = Word2Vec(sg = 0, # 0 para cbow e 1 skip gram \n",
    "               window= 2, # quantas palavras ele vai considerar uma antes e uma depois\n",
    "               vector_size=300, # tamanho do vetor  \n",
    "               min_count= 5, # eliminar as palavras  não recorrentes\n",
    "               alpha= 0.3, # taxa de aprendizgem\n",
    "               min_alpha= 0.007 # minimo de convergencia \n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4261335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "84466\n"
     ]
    }
   ],
   "source": [
    "print(len(titulos_tratados))\n",
    "\n",
    "titulos_tratados = titulos_tratados.dropna().drop_duplicates()\n",
    "print(len(titulos_tratados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a2b253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_lista_tokens = [titulo.split(\" \") for titulo in titulos_tratados.titulo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94c097d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.build_vocab(lista_lista_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09c422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9a13291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_adapt_by_suffix',\n",
       " '_check_corpus_sanity',\n",
       " '_check_training_sanity',\n",
       " '_clear_post_train',\n",
       " '_do_train_epoch',\n",
       " '_do_train_job',\n",
       " '_get_next_alpha',\n",
       " '_get_thread_working_mem',\n",
       " '_job_producer',\n",
       " '_load_specials',\n",
       " '_log_epoch_end',\n",
       " '_log_epoch_progress',\n",
       " '_log_progress',\n",
       " '_log_train_end',\n",
       " '_raw_word_count',\n",
       " '_save_specials',\n",
       " '_scan_vocab',\n",
       " '_smart_save',\n",
       " '_train_epoch',\n",
       " '_train_epoch_corpusfile',\n",
       " '_worker_loop',\n",
       " '_worker_loop_corpusfile',\n",
       " 'add_lifecycle_event',\n",
       " 'add_null_word',\n",
       " 'alpha',\n",
       " 'batch_words',\n",
       " 'build_vocab',\n",
       " 'build_vocab_from_freq',\n",
       " 'cbow_mean',\n",
       " 'comment',\n",
       " 'compute_loss',\n",
       " 'corpus_count',\n",
       " 'corpus_total_words',\n",
       " 'create_binary_tree',\n",
       " 'cum_table',\n",
       " 'effective_min_count',\n",
       " 'epochs',\n",
       " 'estimate_memory',\n",
       " 'get_latest_training_loss',\n",
       " 'hashfxn',\n",
       " 'hs',\n",
       " 'init_sims',\n",
       " 'init_weights',\n",
       " 'layer1_size',\n",
       " 'lifecycle_events',\n",
       " 'load',\n",
       " 'make_cum_table',\n",
       " 'max_final_vocab',\n",
       " 'max_vocab_size',\n",
       " 'min_alpha',\n",
       " 'min_alpha_yet_reached',\n",
       " 'min_count',\n",
       " 'negative',\n",
       " 'ns_exponent',\n",
       " 'null_word',\n",
       " 'predict_output_word',\n",
       " 'prepare_vocab',\n",
       " 'prepare_weights',\n",
       " 'random',\n",
       " 'raw_vocab',\n",
       " 'reset_from',\n",
       " 'running_training_loss',\n",
       " 'sample',\n",
       " 'save',\n",
       " 'scan_vocab',\n",
       " 'score',\n",
       " 'seed',\n",
       " 'seeded_vector',\n",
       " 'sg',\n",
       " 'shrink_windows',\n",
       " 'sorted_vocab',\n",
       " 'syn1neg',\n",
       " 'total_train_time',\n",
       " 'train',\n",
       " 'train_count',\n",
       " 'update_weights',\n",
       " 'vector_size',\n",
       " 'window',\n",
       " 'workers',\n",
       " 'wv']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(w2v_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e768c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84466"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e347b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# iniciando a chamada callback\n",
    "class callback(CallbackAny2Vec):\n",
    "  def __init__(self):\n",
    "    self.epoch = 0\n",
    "\n",
    "  def on_epoch_end(self, model):\n",
    "    loss = model.get_latest_training_loss()\n",
    "    if self.epoch == 0:\n",
    "        print('Loss após a época {}: {}'.format(self.epoch, loss))\n",
    "    else:\n",
    "        print('Loss após a época {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "    self.epoch += 1\n",
    "    self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d26fcf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 0: 442060.40625\n",
      "Loss após a época 1: 103368.09375\n",
      "Loss após a época 2: 42868.875\n",
      "Loss após a época 3: 25541.0\n",
      "Loss após a época 4: 19117.6875\n",
      "Loss após a época 5: 15363.1875\n",
      "Loss após a época 6: 13024.125\n",
      "Loss após a época 7: 11972.875\n",
      "Loss após a época 8: 9449.5625\n",
      "Loss após a época 9: 8579.4375\n",
      "Loss após a época 10: 8727.0\n",
      "Loss após a época 11: 7378.0\n",
      "Loss após a época 12: 6894.5625\n",
      "Loss após a época 13: 6612.6875\n",
      "Loss após a época 14: 6311.8125\n",
      "Loss após a época 15: 6485.5\n",
      "Loss após a época 16: 5642.4375\n",
      "Loss após a época 17: 5201.5625\n",
      "Loss após a época 18: 5429.75\n",
      "Loss após a época 19: 5055.375\n",
      "Loss após a época 20: 5228.9375\n",
      "Loss após a época 21: 4839.9375\n",
      "Loss após a época 22: 4756.625\n",
      "Loss após a época 23: 4443.6875\n",
      "Loss após a época 24: 4409.1875\n",
      "Loss após a época 25: 4279.8125\n",
      "Loss após a época 26: 3766.375\n",
      "Loss após a época 27: 3486.8125\n",
      "Loss após a época 28: 3346.8125\n",
      "Loss após a época 29: 3186.8125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14584190, 16207260)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo.train(lista_lista_tokens,\n",
    "                 total_examples= w2v_modelo.corpus_count,\n",
    "                 epochs= 30,\n",
    "                 compute_loss= True,\n",
    "                 callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5057d530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('facebook', 0.5761243104934692),\n",
       " ('abandono', 0.5467891693115234),\n",
       " ('natura', 0.46859440207481384),\n",
       " ('patriots', 0.4680592715740204),\n",
       " ('gramado', 0.46723243594169617),\n",
       " ('blur', 0.46623989939689636),\n",
       " ('vencedores', 0.46078771352767944),\n",
       " ('atenua', 0.4480825960636139),\n",
       " ('pausa', 0.4403047263622284),\n",
       " ('gravadora', 0.43990692496299744)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo.wv.most_similar(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d22be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5c9a4be",
   "metadata": {},
   "source": [
    "# Skip Gram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7c8192",
   "metadata": {},
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_modelo = Word2Vec(sg = 0, # 0 para cbow e 1 skip gram \n",
    "               window= 2, # quantas palavras ele vai considerar uma antes e uma depois\n",
    "               vector_size=300, # tamanho do vetor  \n",
    "               min_count= 5, # eliminar as palavras  não recorrentes\n",
    "               alpha= 0.3, # taxa de aprendizgem\n",
    "               min_alpha= 0.007 # minimo de convergencia \n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8284ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_modelo_sg = Word2Vec(sg = 0, # 0 para cbow e 1 skip gram \n",
    "               window= 5, # quantas palavras ele vai considerar uma antes e uma depois\n",
    "               vector_size=300, # tamanho do vetor  \n",
    "               min_count= 5, # eliminar as palavras  não recorrentes\n",
    "               alpha= 0.3, # taxa de aprendizgem\n",
    "               min_alpha= 0.007 # minimo de convergencia \n",
    "               )\n",
    "\n",
    "w2v_modelo_sg.build_vocab(lista_lista_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "870d3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss após a época 0: 425160.5625\n",
      "Loss após a época 1: 89842.0625\n",
      "Loss após a época 2: 32921.5\n",
      "Loss após a época 3: 22614.75\n",
      "Loss após a época 4: 16030.0625\n",
      "Loss após a época 5: 12042.3125\n",
      "Loss após a época 6: 10930.0625\n",
      "Loss após a época 7: 9708.4375\n",
      "Loss após a época 8: 8658.0625\n",
      "Loss após a época 9: 8240.1875\n",
      "Loss após a época 10: 7119.5625\n",
      "Loss após a época 11: 6966.5625\n",
      "Loss após a época 12: 6973.25\n",
      "Loss após a época 13: 5846.875\n",
      "Loss após a época 14: 5971.6875\n",
      "Loss após a época 15: 5834.375\n",
      "Loss após a época 16: 5155.5625\n",
      "Loss após a época 17: 5388.625\n",
      "Loss após a época 18: 4869.6875\n",
      "Loss após a época 19: 5300.375\n",
      "Loss após a época 20: 4814.625\n",
      "Loss após a época 21: 4517.125\n",
      "Loss após a época 22: 4448.0\n",
      "Loss após a época 23: 4477.5\n",
      "Loss após a época 24: 4150.0\n",
      "Loss após a época 25: 4057.6875\n",
      "Loss após a época 26: 3982.375\n",
      "Loss após a época 27: 4524.5625\n",
      "Loss após a época 28: 3934.5\n",
      "Loss após a época 29: 3706.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14584037, 16207260)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo_sg.train(lista_lista_tokens,\n",
    "                 total_examples= w2v_modelo.corpus_count,\n",
    "                 epochs= 30,\n",
    "                 compute_loss= True,\n",
    "                 callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95fb8923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apps', 0.4620926082134247),\n",
       " ('trato', 0.43152812123298645),\n",
       " ('feira', 0.4153140187263489),\n",
       " ('unifesp', 0.41097742319107056),\n",
       " ('britânica', 0.41091638803482056),\n",
       " ('nanini', 0.40695855021476746),\n",
       " ('influentes', 0.4050612449645996),\n",
       " ('estilista', 0.39860036969184875),\n",
       " ('acordo', 0.39123257994651794),\n",
       " ('tédio', 0.3904620110988617)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_modelo_sg.wv.most_similar(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28bbae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
