{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7cc43fb",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb1b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "artigo_treino = pd.read_csv(\"../Aula 5 /treino.csv\")\n",
    "artigo_teste = pd.read_csv(\"../Aula 5 /teste.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56d4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U pip setuptools wheel\n",
    "# pip install -U spacy\n",
    "# python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996db166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f264c89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"Adoro a cidade de Caldas novas\"\n",
    "\n",
    "doc = nlp(texto)\n",
    "\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85bad3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Adoro, Caldas novas)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a134fdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fff72e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].is_alpha # é alphanumerico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43511fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].is_stop #Verifica se é uma stopWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02c51682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tratar_textos(doc): #tirar_stopwords e numeros \n",
    "    tokens_validos = []\n",
    "    for token in doc:\n",
    "        e_valido = not token.is_stop and token.is_alpha\n",
    "        if e_valido:\n",
    "            tokens_validos.append(token.text)\n",
    "    if len(tokens_validos) > 2: #word2vec precisa no minimo uma frase com 3 palavras para o contexto\n",
    "        return \" \".join(tokens_validos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c16fa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adoro cidade Caldas'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"Adoro 255255 cidade de Caldas Novas!\"\n",
    "doc = nlp(texto)\n",
    "tratar_textos(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b12115",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_para_tratamento = (titulos.lower() for titulos in artigo_treino.title)\n",
    "\n",
    "textos_tratados = [tratar_textos(doc) for doc in nlp.pipe(textos_para_tratamento,\n",
    "                                                         batch_size=100,\n",
    "                                                         n_process= -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eeed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "titulos_tratados = pd.DataFrame({\"titulo\": textos_tratados})\n",
    "titulos_tratados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c83b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_modelo = Word2Vec(sg = 0, # 0 para cbow e 1 skip gram \n",
    "               window= 2, # quantas palavras ele vai considerar uma antes e uma depois\n",
    "               vector_size=300, # tamanho do vetor  \n",
    "               min_count= 5, # eliminar as palavras  não recorrentes\n",
    "               alpha= 0.3, # taxa de aprendizgem\n",
    "               min_alpha= 0.007 # minimo de convergencia \n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4261335",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(titulos_tratados))\n",
    "\n",
    "titulos_tratados = titulos_tratados.dropna().drop_duplicates()\n",
    "print(len(titulos_tratados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_lista_tokens = [titulo.split(\" \") for titulo in titulos_tratados.titulo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c097d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.build_vocab(lista_lista_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09c422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a13291",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(w2v_modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e768c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# iniciando a chamada callback\n",
    "class callback(CallbackAny2Vec):\n",
    "  def __init__(self):\n",
    "    self.epoch = 0\n",
    "\n",
    "  def on_epoch_end(self, model):\n",
    "    loss = model.get_latest_training_loss()\n",
    "    if self.epoch == 0:\n",
    "        print('Loss após a época {}: {}'.format(self.epoch, loss))\n",
    "    else:\n",
    "        print('Loss após a época {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "    self.epoch += 1\n",
    "    self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26fcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.train(lista_lista_tokens,\n",
    "                 total_examples= w2v_modelo.corpus_count,\n",
    "                 epochs= 30,\n",
    "                 compute_loss= True,\n",
    "                 callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.wv.most_similar(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d22be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5c9a4be",
   "metadata": {},
   "source": [
    "# Skip Gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8284ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_modelo_sg = Word2Vec(sg = 1, # 0 para cbow e 1 skip gram \n",
    "               window= 5, # quantas palavras ele vai considerar uma antes e uma depois\n",
    "               vector_size=300, # tamanho do vetor  \n",
    "               min_count= 5, # eliminar as palavras  não recorrentes\n",
    "               alpha= 0.3, # taxa de aprendizgem\n",
    "               min_alpha= 0.007 # minimo de convergencia \n",
    "               )\n",
    "\n",
    "w2v_modelo_sg.build_vocab(lista_lista_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870d3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo_sg.train(lista_lista_tokens,\n",
    "                 total_examples= w2v_modelo_sg.corpus_count,\n",
    "                 epochs= 30,\n",
    "                 compute_loss= True,\n",
    "                 callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb8923",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo_sg.wv.most_similar(\"google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28bbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6c9f49f",
   "metadata": {},
   "source": [
    "## Exportando modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo.wv.save_word2vec_format(\"./meu_modelo_cbow.txt\", binary= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78caba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_modelo_sg.wv.save_word2vec_format(\"./meu_modelo_skip_gram.txt\", binary= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4706355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "meu_modelo_cbow = KeyedVectors.load_word2vec_format(\"meu_modelo_cbow.txt\")\n",
    "meu_modelo_sg = KeyedVectors.load_word2vec_format(\"meu_modelo_skip_gram.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73682617",
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_modelo_cbow.most_similar(\"ana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88fa43",
   "metadata": {},
   "source": [
    "# Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4076b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenizador(texto):\n",
    "#   tokens_validos = []\n",
    "#   doc = nlp(texto)\n",
    "#   for token in doc:\n",
    "#     e_valido = not token.is_stop and token.is_alpha\n",
    "#     if e_valido:\n",
    "#       tokens_validos.append(token.text.lower())\n",
    "\n",
    "#   return tokens_validos\n",
    "\n",
    "#COMENTEI ESSE TOKENIZADOR POR QUE ELE NÃO FUNCIONA BEM COM SPACY QUE USA PROCESSAMENTO EM LOTES\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def combinacao_de_vetores_por_soma(palavras_numeros,modelo):\n",
    "#   vetor_resultante = np.zeros(300)\n",
    "#   for pn in palavras_numeros:\n",
    "#     try: \n",
    "#       vetor_resultante =+ modelo.get_vector(pn)\n",
    "#     except KeyError:\n",
    "#       pass\n",
    "#   return vetor_resultante\n",
    "# def matriz_vetores(textos,modelo):\n",
    "#     x = len(textos)\n",
    "#     y = 300\n",
    "#     matriz = np.zeros((x,y))\n",
    "\n",
    "#     for i in range(x):\n",
    "#         palavras = tokenizador(textos.iloc[i])\n",
    "#         matriz[i]= combinacao_de_vetores_por_soma(palavras,modelo)\n",
    "#     return matriz\n",
    "# matriz_vetores_treino_cbow = matriz_vetores(artigo_treino.title,meu_modelo)\n",
    "# matriz_vetores_teste_cbow = matriz_vetores(artigo_teste.title,meu_modelo)\n",
    "\n",
    "# COMENTEI O RESTO DO CÓDIGO PQ += ESTA ERRADO NA FUNÇÃO  vetor_resultante =+ modelo.get_vector(pn)\n",
    "#  E A LOGICA NÃO IA FUNCIONAR FAZENDO O TOKENIZADOR DIFERENTE ENTÃO PRECISAVA DE MODIFICAÇÃO\n",
    "# O USO DO SPACY DEVERIA TER SIDO COMO NA FUNÇÃO NO COMEÇO DO NOTEBOOK TRATAR TEXTO\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\n",
    "    \"pt_core_news_sm\",\n",
    "    disable=[\"parser\", \"ner\", \"tagger\", \"textcat\"]\n",
    ")\n",
    "\n",
    "def tokenizador(doc):\n",
    "    return [\n",
    "        token.text.lower()\n",
    "        for token in doc\n",
    "        if token.is_alpha and not token.is_stop\n",
    "    ]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def combinacao_de_vetores_por_soma(palavras, modelo):\n",
    "    vetor_resultante = np.zeros(300)\n",
    "    for pn in palavras:\n",
    "        try:\n",
    "            vetor_resultante += modelo.get_vector(pn)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vetor_resultante\n",
    "\n",
    "\n",
    "def matriz_vetores(docs, modelo):\n",
    "    x = len(docs)\n",
    "    y = 300\n",
    "    matriz = np.zeros((x, y))\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        palavras = tokenizador(doc)\n",
    "        matriz[i] = combinacao_de_vetores_por_soma(palavras, modelo)\n",
    "\n",
    "    return matriz\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_treino = list(nlp.pipe(artigo_treino.title))\n",
    "docs_teste  = list(nlp.pipe(artigo_teste.title))\n",
    "\n",
    "matriz_vetores_treino_cbow = matriz_vetores(docs_treino, meu_modelo_cbow)\n",
    "matriz_vetores_teste_cbow  = matriz_vetores(docs_teste, meu_modelo_cbow)\n",
    "matriz_vetores_treino_sg = matriz_vetores(docs_treino, meu_modelo_sg)\n",
    "matriz_vetores_teste_sg  = matriz_vetores(docs_teste, meu_modelo_sg)\n",
    "\n",
    "# DA FORMA COMENTADA DEMOROU 20 MINUTOS PARA RODAR E AQUI 2MIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39444ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(matriz_vetores_teste_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec72931",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matriz_vetores_treino_cbow.shape)\n",
    "print(matriz_vetores_teste_cbow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63f7ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def classificador(x_treino,y_treino,x_teste, y_teste):\n",
    "    LR = LogisticRegression(max_iter= 800)\n",
    "    LR.fit(x_treino,y_treino)\n",
    "    categorias = LR.predict(x_teste)\n",
    "    CR = classification_report(y_teste,categorias)\n",
    "    print(CR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_cbow = classificador(matriz_vetores_treino_cbow,\n",
    "                        artigo_treino.category,\n",
    "                        matriz_vetores_teste_cbow,\n",
    "                        artigo_teste.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ad8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_skip_gram = classificador(matriz_vetores_treino_sg,\n",
    "                        artigo_treino.category,\n",
    "                        matriz_vetores_teste_sg,\n",
    "                        artigo_teste.category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
